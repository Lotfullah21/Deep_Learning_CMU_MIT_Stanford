{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Back Propagation.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMj+ipUUq8d34HWTM176Pxi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lotfullah21/Deep_Learning_CMU_MIT_Stanford/blob/main/Back_Propagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BXhT7EOjy6AV"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Back propagation is the our neural networks learns, the first step is the forward pass, which contains the affine combination, then applying activation function and it goes for all networks till the end, once these computation reached to the final layer, we compare to our desired value or the target. the second step is calculating the difference between the network output and the target, the difference between is the error and this error is contribution of the past parameters in our network, we will back propagate this value(error) to the network and calculate each layer contribution to the loss or more generally each parameter contribution to the error, we calculate them through parital derivatives."
      ],
      "metadata": {
        "id": "SvGEYcxPy_eF"
      }
    }
  ]
}